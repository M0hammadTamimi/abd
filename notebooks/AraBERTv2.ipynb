{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/M0hammadTamimi/abd/blob/main/notebooks/AraBERTv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets wandb peft torch pandas openpyxl"
      ],
      "metadata": {
        "id": "pn_qPqpVKyeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdSxJ6cbKVZH"
      },
      "outputs": [],
      "source": [
        "# Cell 1: Check GPU and Install Requirements\n",
        "!nvidia-smi\n",
        "!pip install -q transformers datasets wandb peft torch pandas openpyxl\n",
        "\n",
        "# Cell 2: Import Libraries\n",
        "import pandas as pd\n",
        "import wandb\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForQuestionAnswering,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    EarlyStoppingCallback,\n",
        "    default_data_collator\n",
        ")\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from peft import get_peft_model, LoraConfig\n",
        "import torch\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Cell 3: Initialize Wandb and Mount Drive\n",
        "def init_wandb():\n",
        "    if wandb.run is None:\n",
        "        return wandb.init(project=\"huggingface\", entity=\"mohammadtamimi300-hashmite-tech\")\n",
        "    return wandb.run\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "init_wandb()\n",
        "\n",
        "# Cell 4: Load Dataset\n",
        "df = pd.read_excel('/content/datasetQA.xlsx')\n",
        "print(f\"Dataset loaded with {len(df)} rows\")\n",
        "\n",
        "# Cell 5: Initialize Tokenizer and Preprocessing\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"aubmindlab/bert-base-arabertv2\")\n",
        "\n",
        "def improved_preprocess_function(examples):\n",
        "    context = str(examples['context'])\n",
        "    question = str(examples['question'])\n",
        "    answer = str(examples['answer'])\n",
        "\n",
        "    if not context or not answer:\n",
        "        return None\n",
        "\n",
        "    # Normalize whitespace\n",
        "    context = ' '.join(context.split())\n",
        "    answer = ' '.join(answer.split())\n",
        "\n",
        "    start_position = context.find(answer)\n",
        "    if start_position == -1:\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Encode the inputs\n",
        "        encoding = tokenizer(\n",
        "            question,\n",
        "            context,\n",
        "            max_length=512,\n",
        "            truncation='only_second',\n",
        "            padding='max_length',\n",
        "            return_offsets_mapping=True,\n",
        "            return_overflowing_tokens=True,\n",
        "            stride=128\n",
        "        )\n",
        "\n",
        "        # Convert character positions to token positions\n",
        "        offsets = encoding['offset_mapping'][0]\n",
        "        start_token = None\n",
        "        end_token = None\n",
        "\n",
        "        for idx, (start, end) in enumerate(offsets):\n",
        "            if start <= start_position < end:\n",
        "                start_token = idx\n",
        "            if start < start_position + len(answer) <= end:\n",
        "                end_token = idx\n",
        "                break\n",
        "\n",
        "        if start_token is None or end_token is None:\n",
        "            return None\n",
        "\n",
        "        # Get first window and prepare final encoding\n",
        "        encoding = {k: v[0] for k, v in encoding.items()}\n",
        "        encoding['start_positions'] = start_token\n",
        "        encoding['end_positions'] = end_token\n",
        "\n",
        "        return encoding\n",
        "    except Exception as e:\n",
        "        print(f\"Error in preprocessing: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Cell 6: Process and Split Dataset\n",
        "print(\"Processing dataset...\")\n",
        "tokenized_datasets = df.apply(improved_preprocess_function, axis=1).dropna().tolist()\n",
        "print(f\"Processed {len(tokenized_datasets)} valid examples\")\n",
        "\n",
        "# Split dataset\n",
        "train_data, val_data = train_test_split(tokenized_datasets, test_size=0.1, random_state=42)\n",
        "train_dataset = Dataset.from_list(train_data)\n",
        "val_dataset = Dataset.from_list(val_data)\n",
        "print(f\"Train set: {len(train_dataset)} examples\")\n",
        "print(f\"Validation set: {len(val_dataset)} examples\")\n",
        "\n",
        "# Cell 7: Initialize Model and Configure LoRA\n",
        "print(\"Initializing model...\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\n",
        "    \"aubmindlab/bert-base-arabertv2\",\n",
        "    return_dict=True\n",
        ")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"QUESTION_ANS\",\n",
        "    target_modules=[\"query\", \"key\", \"value\"],\n",
        "    bias=\"none\",\n",
        "    modules_to_save=[\"qa_outputs\"]\n",
        ")\n",
        "\n",
        "# Apply LoRA to model\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Cell 8: Configure Training\n",
        "output_dir = r'D:/training_model_output'  # Updated output directory\n",
        "logging_dir = r'D:/training_model_output/logs'  # Updated logging directory\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(logging_dir, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"steps\",  # Updated from evaluation_strategy\n",
        "    eval_steps=50,\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=logging_dir,\n",
        "    report_to=\"wandb\",\n",
        "    fp16=True,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=True\n",
        ")\n",
        "\n",
        "# Cell 9: Initialize Trainer and Start Training\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=default_data_collator,\n",
        ")\n",
        "\n",
        "# Add early stopping\n",
        "early_stopping = EarlyStoppingCallback(\n",
        "    early_stopping_patience=3,\n",
        "    early_stopping_threshold=0.01\n",
        ")\n",
        "trainer.add_callback(early_stopping)\n",
        "\n",
        "# Train with proper error handling\n",
        "print(\"Starting training...\")\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"Training completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Training error occurred: {str(e)}\")\n",
        "    print(\"Saving checkpoint model...\")\n",
        "    model.save_pretrained(\"D:/training_model_output/checkpoint_model\")\n",
        "\n",
        "# Cell 10: Evaluate and Save Model\n",
        "print(\"\\nRunning final evaluation...\")\n",
        "try:\n",
        "    # Ensure wandb is initialized\n",
        "    init_wandb()\n",
        "\n",
        "    # Run evaluation\n",
        "    validation_output = trainer.evaluate()\n",
        "    print(\"\\nValidation Results:\", validation_output)\n",
        "\n",
        "    # Save the model\n",
        "    print(\"\\nSaving final model...\")\n",
        "    model.save_pretrained(output_dir)  # Saving model to the specified path\n",
        "    tokenizer.save_pretrained(output_dir)  # Saving tokenizer to the specified path\n",
        "    print(f\"Model saved successfully to {output_dir}!\")\n",
        "\n",
        "    # Create zip file\n",
        "    import zipfile\n",
        "    zip_file_path = output_dir + \".zip\"\n",
        "    with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for root, dirs, files in os.walk(output_dir):\n",
        "            for file in files:\n",
        "                zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), output_dir))\n",
        "    print(\"Model zipped successfully!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error in final steps: {str(e)}\")\n",
        "finally:\n",
        "    # Always ensure wandb is properly closed\n",
        "    if wandb.run:\n",
        "        wandb.finish()\n",
        "\n",
        "# Cell 11: Optional - Save to Drive\n",
        "# Ensure that this is pointing to your desired destination, here the path is for Google Drive but adjust it as needed."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model = model_name)\n",
        "unmasker(preprocessed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okM9mZBEKbXy",
        "outputId": "1cd9b7e4-8849-4290-a346-b906fc788943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.375786691904068,\n",
              "  'token': 1875,\n",
              "  'token_str': 'بيروت',\n",
              "  'sequence': 'عاصم لبنان هي بيروت'},\n",
              " {'score': 0.08452503383159637,\n",
              "  'token': 48,\n",
              "  'token_str': '.',\n",
              "  'sequence': 'عاصم لبنان هي.'},\n",
              " {'score': 0.07384563982486725,\n",
              "  'token': 2314,\n",
              "  'token_str': 'دمشق',\n",
              "  'sequence': 'عاصم لبنان هي دمشق'},\n",
              " {'score': 0.05577431991696358,\n",
              "  'token': 59,\n",
              "  'token_str': ':',\n",
              "  'sequence': 'عاصم لبنان هي :'},\n",
              " {'score': 0.042271003127098083,\n",
              "  'token': 2912,\n",
              "  'token_str': 'باريس',\n",
              "  'sequence': 'عاصم لبنان هي باريس'}]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}